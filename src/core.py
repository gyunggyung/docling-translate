"""
src/core.py
===========
ë¬¸ì„œ ì²˜ë¦¬ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1.  **ë¬¸ì„œ ë³€í™˜**: Doclingì„ ì‚¬ìš©í•˜ì—¬ PDF, DOCX ë“±ì˜ ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2.  **í…ìŠ¤íŠ¸ ìˆ˜ì§‘**: ë³€í™˜ëœ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ì™€ ìº¡ì…˜ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
3.  **ë²ˆì—­ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ `src.translation` íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ë²ˆì—­í•©ë‹ˆë‹¤.
4.  **HTML ìƒì„±**: `src.html_generator`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ ê²°ê³¼ê°€ í¬í•¨ëœ ì¸í„°ë™í‹°ë¸Œ HTMLì„ ìƒì„±í•©ë‹ˆë‹¤.
5.  **í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬**: txt, md, py ë“± í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ìŠ¤ë§ˆíŠ¸ ë²ˆì—­ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import time
import logging
import nltk
from pathlib import Path
from datetime import datetime
from typing import Optional, Callable
import multiprocessing

# [Issue #92] Docling CPU ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
# Docling ë° PyTorch/OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ê¸° ì „ì— í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
try:
    # CPU ë¬¼ë¦¬ ì½”ì–´ ìˆ˜ ë˜ëŠ” ë…¼ë¦¬ ì½”ì–´ ìˆ˜ í™•ì¸ (ê°€ëŠ¥í•˜ë©´ ë¬¼ë¦¬ ì½”ì–´ ê¶Œì¥ë˜ë‚˜ multiprocessingì€ ë…¼ë¦¬ ì½”ì–´ ë°˜í™˜)
    cpu_count = str(multiprocessing.cpu_count())
    
    # ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì„¤ì • (ì‚¬ìš©ì ì§€ì • ê°’ ì¡´ì¤‘)
    if "OMP_NUM_THREADS" not in os.environ:
        os.environ["OMP_NUM_THREADS"] = cpu_count
    
    # ì¶”ê°€ ê°€ì†í™” ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜
    if "MKL_NUM_THREADS" not in os.environ:
        os.environ["MKL_NUM_THREADS"] = cpu_count
    if "TORCH_NUM_THREADS" not in os.environ:
        os.environ["TORCH_NUM_THREADS"] = cpu_count
        
    logging.info(f"[Optimization] CPU Optimization Enabled: Threads set to {cpu_count}")
except Exception as e:
    logging.warning(f"[Optimization] Failed to set CPU threads: {e}")

from docling.document_converter import (
    DocumentConverter,
    PdfFormatOption,
    WordFormatOption,
    PowerpointFormatOption,
    HTMLFormatOption,
    ImageFormatOption
)
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode
from docling_core.types.doc import DoclingDocument, TextItem, TableItem, PictureItem

# [NEW] pypdfium2 ë°±ì—”ë“œ import (Issue #100 - ì†ë„ ìµœì í™”)
# Fast ëª¨ë“œì—ì„œ ì‚¬ìš©í•˜ë©´ 3-5ë°° ì†ë„ í–¥ìƒ
try:
    from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend
    PYPDFIUM_AVAILABLE = True
except ImportError:
    PYPDFIUM_AVAILABLE = False
    logging.warning("[Speed] PyPdfiumDocumentBackend not available. Fast mode will use default backend.")

from src.benchmark import global_benchmark as bench
from src.translation import create_translator
from src.html_generator import generate_html_content
from src.utils import ensure_nltk_resources
from src.text_parser import TextFileParser, is_text_file
from src.text_html_generator import generate_text_html, get_file_type_display, generate_code_file_html

# ì§„í–‰ë¥  ì½œë°± íƒ€ì… ì •ì˜ (float: ì§„í–‰ë¥  0.0~1.0, str: ìƒíƒœ ë©”ì‹œì§€)
ProgressCallback = Callable[[float, str], None]

def create_converter(speed_mode: str = "balanced") -> DocumentConverter:
    """
    Docling DocumentConverterë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        speed_mode: "fast" | "balanced" 
            - fast: pypdfium2 ë°±ì—”ë“œ, TableFormerMode.FAST, ì´ë¯¸ì§€ ìƒì„± ë¹„í™œì„±í™”
            - balanced: ê¸°ë³¸ ë°±ì—”ë“œ, TableFormerMode.ACCURATE, ì´ë¯¸ì§€ ìƒì„± í™œì„±í™”
    
    Returns:
        DocumentConverter: ì„¤ì •ëœ ë¬¸ì„œ ë³€í™˜ê¸° ì¸ìŠ¤í„´ìŠ¤
    """
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = True
    
    # [NEW] ìˆ˜ì‹ ì¶”ì¶œ í™œì„±í™” (Issue #102)
    pipeline_options.do_formula_enrichment = True
    
    # [NEW] ì†ë„ ëª¨ë“œì— ë”°ë¥¸ ì„¤ì • ë¶„ê¸° (Issue #100)
    if speed_mode == "fast":
        # Fast ëª¨ë“œ: pypdfium2 ë°±ì—”ë“œ + TableFormerMode.FASTë§Œ ì ìš©
        # ì´ë¯¸ì§€/í•´ìƒë„ëŠ” Balancedì™€ ë™ì¼í•˜ê²Œ ìœ ì§€
        pipeline_options.table_structure_options.mode = TableFormerMode.FAST
        pipeline_options.generate_picture_images = True   # ì´ë¯¸ì§€ ìœ ì§€
        pipeline_options.generate_table_images = True     # í‘œ ì´ë¯¸ì§€ ìœ ì§€
        pipeline_options.images_scale = 2.0               # ê³ í•´ìƒë„ ìœ ì§€
    else:
        # Balanced ëª¨ë“œ: í’ˆì§ˆ ìš°ì„  (ê¸°ë³¸ê°’)
        pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE
        pipeline_options.generate_picture_images = True
        pipeline_options.generate_table_images = True
        pipeline_options.images_scale = 2.0  # ê³ í•´ìƒë„

    # [NEW] ì†ë„ ëª¨ë“œì— ë”°ë¥¸ PDF ë°±ì—”ë“œ ì„ íƒ (Issue #100)
    if speed_mode == "fast" and PYPDFIUM_AVAILABLE:
        # Fast ëª¨ë“œ: pypdfium2 ë°±ì—”ë“œ (3-5ë°° ì†ë„ í–¥ìƒ)
        pdf_format_option = PdfFormatOption(
            pipeline_options=pipeline_options,
            backend=PyPdfiumDocumentBackend
        )
    else:
        # Balanced ëª¨ë“œ: ê¸°ë³¸ ë°±ì—”ë“œ (docling-parse-v4)
        pdf_format_option = PdfFormatOption(pipeline_options=pipeline_options)

    return DocumentConverter(
        allowed_formats=[
            InputFormat.PDF,
            InputFormat.DOCX,
            InputFormat.PPTX,
            InputFormat.HTML,
            InputFormat.IMAGE,
        ],
        format_options={
            InputFormat.PDF: pdf_format_option,
            InputFormat.DOCX: WordFormatOption(),
            InputFormat.PPTX: PowerpointFormatOption(),
            InputFormat.HTML: HTMLFormatOption(),
            InputFormat.IMAGE: ImageFormatOption(),
        },
    )


# UI ë©”ì‹œì§€ ë‹¤êµ­ì–´ ì§€ì›
PROGRESS_MESSAGES = {
    "ko": {
        "analyzing": "ğŸ“„ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ë³€í™˜ ì¤‘... ({file_name})",
        "error_search": "âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({file_name})",
        "error_convert": "âŒ ì˜¤ë¥˜: ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨ ({file_name})",
        "extracting": "ğŸ“ í…ìŠ¤íŠ¸ ë° ìº¡ì…˜ ì¶”ì¶œ ì¤‘... ({file_name})",
        "translating_start": "ğŸ¤– ë²ˆì—­ ì‹œì‘... ({count} ë¬¸ì¥)",
        "translating_progress": "ğŸ¤– ë²ˆì—­ ì¤‘... {msg}",
        "saving": "ğŸ’¾ ê²°ê³¼ íŒŒì¼ ìƒì„± ë° ì´ë¯¸ì§€ ì €ì¥ ì¤‘... ({file_name})",
        "saving_progress": "ğŸ’¾ {msg}",
        "done": "âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ({file_name})"
    },
    "en": {
        "analyzing": "ğŸ“„ Analyzing document structure... ({file_name})",
        "error_search": "âŒ Error: File not found ({file_name})",
        "error_convert": "âŒ Error: Document conversion failed ({file_name})",
        "extracting": "ğŸ“ Extracting text and captions... ({file_name})",
        "translating_start": "ğŸ¤– Starting translation... ({count} sentences)",
        "translating_progress": "ğŸ¤– Translating... {msg}",
        "saving": "ğŸ’¾ Generating result file and saving images... ({file_name})",
        "saving_progress": "ğŸ’¾ {msg}",
        "done": "âœ… All tasks completed! ({file_name})"
    }
}


def process_text_file(
    file_path: str,
    source_lang: str,
    target_lang: str,
    engine: str,
    max_workers: int = 1,
    progress_cb: Optional[ProgressCallback] = None,
    ui_lang: str = "ko",
) -> dict:
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ ì „ìš© ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    txt, md, py, js ë“±ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ë²ˆì—­ ëŒ€ìƒ ì˜ì—­ë§Œ ì¶”ì¶œí•˜ê³ ,
    ë²ˆì—­ í›„ ì¸í„°ë™í‹°ë¸Œ HTMLì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        file_path: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ
        target_lang: ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ
        engine: ë²ˆì—­ ì—”ì§„
        max_workers: ë³‘ë ¬ ì›Œì»¤ ìˆ˜
        progress_cb: ì§„í–‰ë¥  ì½œë°±
        ui_lang: UI ì–¸ì–´
        
    Returns:
        ê²°ê³¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (output_dir, html_path)
    """
    ensure_nltk_resources()
    
    msgs = PROGRESS_MESSAGES.get(ui_lang, PROGRESS_MESSAGES["ko"])
    file_name = Path(file_path).name
    
    bench.start(f"Total Process (Text): {file_name}")
    
    if progress_cb:
        progress_cb(0.05, f"ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ë¶„ì„ ì¤‘... ({file_name})")
    
    # 1. íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
    if not os.path.exists(file_path):
        logging.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        if progress_cb:
            progress_cb(1.0, msgs["error_search"].format(file_name=file_name))
        return {}
    
    # 2. ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    base_filename = Path(file_path).stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("output") / f"{base_filename}_{source_lang}_to_{target_lang}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logging.info(f"[{file_name}] í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (ì—”ì§„: {engine})")
    
    # 3. í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹±
    if progress_cb:
        progress_cb(0.10, f"ğŸ“ íŒŒì¼ íŒŒì‹± ì¤‘... ({file_name})")
    
    parser = TextFileParser()
    try:
        segments = parser.parse(Path(file_path))
    except Exception as e:
        logging.error(f"[{file_name}] íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}", exc_info=True)
        if progress_cb:
            progress_cb(1.0, f"âŒ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {}
    
    # ë²ˆì—­ ëŒ€ìƒ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    translatable_texts = parser.get_translatable_texts(segments)
    unique_texts = list(set(translatable_texts))
    
    logging.info(f"[{file_name}] ì„¸ê·¸ë¨¼íŠ¸ {len(segments)}ê°œ, ë²ˆì—­ ëŒ€ìƒ {len(unique_texts)}ê°œ")
    
    if progress_cb:
        progress_cb(0.20, msgs["translating_start"].format(count=len(unique_texts)))
    
    # 4. ë²ˆì—­ ì‹¤í–‰
    bench.start(f"Translation (Text): {file_name}")
    t_trans_start = time.time()
    
    TRANSLATE_BASE = 0.20
    TRANSLATE_SPAN = 0.60
    
    def _translate_progress(local_ratio: float, msg: str):
        if progress_cb:
            global_ratio = TRANSLATE_BASE + TRANSLATE_SPAN * local_ratio
            progress_cb(global_ratio, msgs["translating_progress"].format(msg=msg))
    
    translator = create_translator(engine)
    translated_results = translator.translate_batch(
        unique_texts,
        src=source_lang,
        dest=target_lang,
        max_workers=max_workers,
        progress_cb=_translate_progress
    )
    
    t_trans_end = time.time()
    
    # ë²ˆì—­ ë§µ ìƒì„±
    translation_map = dict(zip(unique_texts, translated_results))
    
    bench.end(f"Translation (Text): {file_name}")
    logging.info(f"[{file_name}] ë²ˆì—­ ì™„ë£Œ ({t_trans_end - t_trans_start:.2f}ì´ˆ)")
    
    # 5. HTML ìƒì„±
    if progress_cb:
        progress_cb(0.85, msgs["saving"].format(file_name=file_name))
    
    ext = Path(file_path).suffix.lstrip('.').lower()
    file_type = get_file_type_display(ext)
    
    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ê¸°
    is_markdown = ext in ('md', 'markdown')
    is_code_file = ext in ('py', 'pyw', 'js', 'jsx', 'ts', 'tsx', 'c', 'h', 'cpp', 'hpp', 'cc', 'cxx', 'cs', 'java', 'kt', 'kts', 'go', 'rs', 'swift', 'sh', 'bash', 'zsh')
    
    GEN_BASE = 0.85
    GEN_SPAN = 0.15
    
    def _gen_progress(local_ratio: float, msg: str):
        if progress_cb:
            global_ratio = GEN_BASE + GEN_SPAN * local_ratio
            progress_cb(global_ratio, msgs["saving_progress"].format(msg=msg))
    
    if is_code_file:
        # ì½”ë“œ íŒŒì¼: ì›ë³¸ ì½”ë“œ êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ ì£¼ì„ë§Œ ë²ˆì—­
        original_content = Path(file_path).read_text(encoding='utf-8', errors='ignore')
        html_content = generate_code_file_html(
            file_name=file_name,
            original_content=original_content,
            segments=segments,
            translation_map=translation_map,
            file_type=file_type,
            progress_cb=_gen_progress
        )
    else:
        # ë§ˆí¬ë‹¤ìš´/ì¼ë°˜ í…ìŠ¤íŠ¸
        html_content = generate_text_html(
            file_name=file_name,
            segments=segments,
            translation_map=translation_map,
            file_type=file_type,
            is_markdown=is_markdown,
            progress_cb=_gen_progress
        )
    
    path_html = output_dir / f"{base_filename}_interactive.html"
    with open(path_html, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    if progress_cb:
        progress_cb(1.0, msgs["done"].format(file_name=file_name))
    
    bench.end(f"Total Process (Text): {file_name}")
    logging.info(f"[{file_name}] í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {output_dir}")
    
    return {
        "output_dir": output_dir,
        "html_path": path_html
    }


def process_single_file(
    file_path: str,
    converter: DocumentConverter,
    source_lang: str,
    target_lang: str,
    engine: str,
    max_workers: int = 1,
    progress_cb: Optional[ProgressCallback] = None,
    ui_lang: str = "ko",
) -> dict:
    """
    ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•µì‹¬ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    ë‹¨ê³„:
    1. íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ ë° ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
    2. Doclingì„ ì‚¬ìš©í•œ ë¬¸ì„œ ë³€í™˜ (PDF/DOCX ë“± -> DoclingDocument)
    3. í…ìŠ¤íŠ¸ ë° ìº¡ì…˜ ì¶”ì¶œ (Collection)
    4. ì„ íƒí•œ ì—”ì§„ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ë²ˆì—­ (Translation)
    5. ë²ˆì—­ëœ ë‚´ìš©ì„ í¬í•¨í•œ ì¸í„°ë™í‹°ë¸Œ HTML ìƒì„± (HTML Generation)
    
    Args:
        file_path (str): ì²˜ë¦¬í•  íŒŒì¼ì˜ ê²½ë¡œ
        converter (DocumentConverter): Docling ë³€í™˜ê¸° ì¸ìŠ¤í„´ìŠ¤
        source_lang (str): ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'en')
        target_lang (str): ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko')
        engine (str): ì‚¬ìš©í•  ë²ˆì—­ ì—”ì§„ ('google', 'deepl', 'gemini', 'openai')
        max_workers (int): ë³‘ë ¬ ë²ˆì—­ ì‹œ ì‚¬ìš©í•  ì›Œì»¤ ìˆ˜
        progress_cb (Optional[ProgressCallback]): ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜
        ui_lang (str): UI í‘œì‹œ ì–¸ì–´ ('ko' or 'en')

    Returns:
        dict: ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ (output_dir, html_path í¬í•¨). ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬.
    """
    ensure_nltk_resources()
    
    # UI ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ko)
    msgs = PROGRESS_MESSAGES.get(ui_lang, PROGRESS_MESSAGES["ko"])

    file_name = Path(file_path).name
    bench.start(f"Total Process: {file_name}")

    if progress_cb:
        progress_cb(0.02, msgs["analyzing"].format(file_name=file_name))

    # 0. í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš° ë³„ë„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬
    if is_text_file(file_path):
        logging.info(f"[{file_name}] í…ìŠ¤íŠ¸ íŒŒì¼ ê°ì§€ë¨, í…ìŠ¤íŠ¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „í™˜")
        return process_text_file(
            file_path=file_path,
            source_lang=source_lang,
            target_lang=target_lang,
            engine=engine,
            max_workers=max_workers,
            progress_cb=progress_cb,
            ui_lang=ui_lang
        )
    
    # 1. ì…ë ¥ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
    if not os.path.exists(file_path):
        logging.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        if progress_cb:
            progress_cb(1.0, msgs["error_search"].format(file_name=file_name))
        return {}

    # 2. ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    # í´ë”ëª… í˜•ì‹: {íŒŒì¼ëª…}_{ì¶œë°œì–¸ì–´}_to_{ë„ì°©ì–¸ì–´}_{íƒ€ì„ìŠ¤íƒ¬í”„}
    base_filename = Path(file_path).stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("output") / f"{base_filename}_{source_lang}_to_{target_lang}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logging.info(f"[{file_name}] ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ (ì—”ì§„: {engine})")

    # 3. Docling ë³€í™˜
    bench.start(f"Conversion: {file_name}")
    logging.info(f"[{file_name}] ë¬¸ì„œ ë³€í™˜ ì¤‘...")
    try:
        doc: DoclingDocument = converter.convert(file_path).document
    except Exception as e:
        logging.error(f"[{file_name}] ë¬¸ì„œ ë³€í™˜ ì˜¤ë¥˜: {e}", exc_info=True)
        if progress_cb:
            progress_cb(1.0, msgs["error_convert"].format(file_name=file_name))
        return {}
    bench.end(f"Conversion: {file_name}")
    logging.info(f"[{file_name}] ë¬¸ì„œ ë³€í™˜ ì„±ê³µ.")

    if progress_cb:
        progress_cb(0.20, msgs["extracting"].format(file_name=file_name))

    # 4. í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ë²ˆì—­
    bench.start(f"Translation & Save: {file_name}")
    logging.info(f"[{file_name}] í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ì¼ê´„ ë²ˆì—­ ì¤€ë¹„... (Workers: {max_workers})")

    # --- Phase 1: Collection (í…ìŠ¤íŠ¸ ìˆ˜ì§‘) ---
    all_sentences = []
    doc_items = []
    
    # ë¬¸ì„œë¥¼ ìˆœíšŒí•˜ë©° í…ìŠ¤íŠ¸ ì•„ì´í…œê³¼ ìº¡ì…˜ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    for item, _ in doc.iterate_items():
        doc_items.append((item, _))
        
        if isinstance(item, TextItem):
            if item.text and item.text.strip():
                # NLTKë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                sentences = nltk.sent_tokenize(item.text)
                all_sentences.extend(sentences)
        elif isinstance(item, (TableItem, PictureItem)):
            orig_caption = item.caption_text(doc)
            if orig_caption:
                all_sentences.append(orig_caption)
            
            # [NEW] í‘œ ì…€ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (pandas DataFrame í™œìš©)
            # TableItemì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë²ˆì—­ ëŒ€ìƒì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
            if isinstance(item, TableItem):
                try:
                    # [FIX] deprecated API ìˆ˜ì • (Issue #102)
                    # export_to_dataframe()ì— doc ì¸ì ì¶”ê°€
                    df = item.export_to_dataframe(doc)
                    # ë°ì´í„°í”„ë ˆì„ì˜ ëª¨ë“  ì…€ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ìˆ˜ì§‘
                    for text in df.values.flatten():
                        if isinstance(text, str) and text.strip():
                            all_sentences.append(text)
                    # ì»¬ëŸ¼ í—¤ë”ë„ ìˆ˜ì§‘
                    for col in df.columns:
                        if isinstance(col, str) and col.strip():
                            all_sentences.append(col)
                except Exception as e:
                    logging.warning(f"[{file_name}] í‘œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ(ë¬´ì‹œë¨): {e}")

    # ì¤‘ë³µ ë¬¸ì¥ ì œê±° (ë²ˆì—­ ë¹„ìš© ì ˆê°)
    unique_sentences = list(set(all_sentences))
    logging.info(f"[{file_name}] ì´ {len(all_sentences)}ê°œ ë¬¸ì¥ ìˆ˜ì§‘ (ê³ ìœ  ë¬¸ì¥: {len(unique_sentences)}ê°œ)")

    if progress_cb:
        progress_cb(0.25, msgs["translating_start"].format(count=len(unique_sentences)))

    # --- Phase 2: Translation (ë²ˆì—­) ---
    t_trans_start = time.time()
    
    # ì§„í–‰ë¥  ê³„ì‚°ì„ ìœ„í•œ ìƒìˆ˜ (ë²ˆì—­ ë¹„ì¤‘ 60%)
    TRANSLATE_BASE = 0.25
    TRANSLATE_SPAN = 0.60

    # ë²ˆì—­ ì—”ì§„ì˜ ì§„í–‰ë¥  ì½œë°± ë˜í¼
    def _translate_progress(local_ratio: float, msg: str):
        if progress_cb:
            global_ratio = TRANSLATE_BASE + TRANSLATE_SPAN * local_ratio
            progress_cb(global_ratio, msgs["translating_progress"].format(msg=msg))

    # Translator ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¼ê´„ ë²ˆì—­ ì‹¤í–‰
    translator = create_translator(engine)
    translated_results = translator.translate_batch(
        unique_sentences,
        src=source_lang,
        dest=target_lang,
        max_workers=max_workers,
        progress_cb=_translate_progress
    )

    t_trans_end = time.time()
    
    # ì›ë¬¸-ë²ˆì—­ë¬¸ ë§¤í•‘ ìƒì„±
    translation_map = dict(zip(unique_sentences, translated_results))

    # ë²¤ì¹˜ë§ˆí¬ í†µê³„ ê¸°ë¡
    total_chars = sum(len(s) for s in unique_sentences)
    bench.add_stat(
        "Translation (Sentences)",
        t_trans_end - t_trans_start,
        count=len(unique_sentences),
        volume=total_chars,
        unit="chars",
    )
    logging.info(f"[{file_name}] ì¼ê´„ ë²ˆì—­ ì™„ë£Œ ({t_trans_end - t_trans_start:.2f}ì´ˆ)")

    # --- Phase 3: HTML Generation (HTML ìƒì„±) ---
    if progress_cb:
        progress_cb(0.85, msgs["saving"].format(file_name=file_name))

    path_html = output_dir / f"{base_filename}_interactive.html"
    
    # HTML ìƒì„± ì‹œ ì´ë¯¸ì§€ ì €ì¥ ì§„í–‰ë¥  ë°˜ì˜ (ë‚˜ë¨¸ì§€ 15%)
    GEN_BASE = 0.85
    GEN_SPAN = 0.15
    
    def _gen_progress(local_ratio: float, msg: str):
        if progress_cb:
            global_ratio = GEN_BASE + GEN_SPAN * local_ratio
            progress_cb(global_ratio, msgs["saving_progress"].format(msg=msg))

    html_content = generate_html_content(
        doc,
        doc_items,
        translation_map,
        output_dir,
        base_filename,
        progress_cb=_gen_progress
    )

    with open(path_html, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    if progress_cb:
        progress_cb(1.0, msgs["done"].format(file_name=file_name))
    
    bench.end(f"Translation & Save: {file_name}")
    bench.end(f"Total Process: {file_name}")
    logging.info(f"[{file_name}] íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_dir}")
    
    return {
        "output_dir": output_dir,
        "html_path": path_html
    }

def process_document(
    file_path: str,
    converter: DocumentConverter,
    source_lang: str = "en",
    dest_lang: str = "ko",
    engine: str = "google",
    max_workers: int = 8,
    progress_cb: Optional[ProgressCallback] = None,
    ui_lang: str = "ko",
) -> dict:
    """
    ì™¸ë¶€(app.py, main.py)ì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•œ í¸ì˜ì„± ë˜í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    process_single_fileì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    return process_single_file(
        file_path=file_path,
        converter=converter,
        source_lang=source_lang,
        target_lang=dest_lang,
        engine=engine,
        max_workers=max_workers,
        progress_cb=progress_cb,
        ui_lang=ui_lang
    )
