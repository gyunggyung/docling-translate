"""
src/core.py
===========
ë¬¸ì„œ ì²˜ë¦¬ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1.  **ë¬¸ì„œ ë³€í™˜**: Doclingì„ ì‚¬ìš©í•˜ì—¬ PDF, DOCX ë“±ì˜ ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2.  **í…ìŠ¤íŠ¸ ìˆ˜ì§‘**: ë³€í™˜ëœ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ì™€ ìº¡ì…˜ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
3.  **ë²ˆì—­ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ `src.translation` íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ë²ˆì—­í•©ë‹ˆë‹¤.
4.  **HTML ìƒì„±**: `src.html_generator`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ ê²°ê³¼ê°€ í¬í•¨ëœ ì¸í„°ë™í‹°ë¸Œ HTMLì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import time
import logging
import nltk
from pathlib import Path
from datetime import datetime
from typing import Optional, Callable

from docling.document_converter import (
    DocumentConverter,
    PdfFormatOption,
    WordFormatOption,
    PowerpointFormatOption,
    HTMLFormatOption,
    ImageFormatOption
)
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling_core.types.doc import DoclingDocument, TextItem, TableItem, PictureItem

from src.benchmark import global_benchmark as bench
from src.translation import create_translator
from src.html_generator import generate_html_content
from src.utils import ensure_nltk_resources

# ì§„í–‰ë¥  ì½œë°± íƒ€ì… ì •ì˜ (float: ì§„í–‰ë¥  0.0~1.0, str: ìƒíƒœ ë©”ì‹œì§€)
ProgressCallback = Callable[[float, str], None]

def create_converter() -> DocumentConverter:
    """
    Docling DocumentConverterë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    
    PDF ë³€í™˜ ì‹œ OCRì€ ë¹„í™œì„±í™”í•˜ê³ , í…Œì´ë¸” êµ¬ì¡° ë¶„ì„ ë° ì´ë¯¸ì§€ ì¶”ì¶œì„ í™œì„±í™”í•©ë‹ˆë‹¤.
    ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ì€ 2.0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    
    Returns:
        DocumentConverter: ì„¤ì •ëœ ë¬¸ì„œ ë³€í™˜ê¸° ì¸ìŠ¤í„´ìŠ¤
    """
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = True
    pipeline_options.generate_picture_images = True
    pipeline_options.generate_table_images = True
    pipeline_options.images_scale = 2.0

    return DocumentConverter(
        allowed_formats=[
            InputFormat.PDF,
            InputFormat.DOCX,
            InputFormat.PPTX,
            InputFormat.HTML,
            InputFormat.IMAGE,
        ],
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options),
            InputFormat.DOCX: WordFormatOption(),
            InputFormat.PPTX: PowerpointFormatOption(),
            InputFormat.HTML: HTMLFormatOption(),
            InputFormat.IMAGE: ImageFormatOption(),
        },
    )

def process_single_file(
    file_path: str,
    converter: DocumentConverter,
    source_lang: str,
    target_lang: str,
    engine: str,
    max_workers: int = 1,
    progress_cb: Optional[ProgressCallback] = None,
) -> dict:
    """
    ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•µì‹¬ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    ë‹¨ê³„:
    1. íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ ë° ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
    2. Doclingì„ ì‚¬ìš©í•œ ë¬¸ì„œ ë³€í™˜ (PDF/DOCX ë“± -> DoclingDocument)
    3. í…ìŠ¤íŠ¸ ë° ìº¡ì…˜ ì¶”ì¶œ (Collection)
    4. ì„ íƒí•œ ì—”ì§„ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ë²ˆì—­ (Translation)
    5. ë²ˆì—­ëœ ë‚´ìš©ì„ í¬í•¨í•œ ì¸í„°ë™í‹°ë¸Œ HTML ìƒì„± (HTML Generation)
    
    Args:
        file_path (str): ì²˜ë¦¬í•  íŒŒì¼ì˜ ê²½ë¡œ
        converter (DocumentConverter): Docling ë³€í™˜ê¸° ì¸ìŠ¤í„´ìŠ¤
        source_lang (str): ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'en')
        target_lang (str): ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko')
        engine (str): ì‚¬ìš©í•  ë²ˆì—­ ì—”ì§„ ('google', 'deepl', 'gemini', 'openai')
        max_workers (int): ë³‘ë ¬ ë²ˆì—­ ì‹œ ì‚¬ìš©í•  ì›Œì»¤ ìˆ˜
        progress_cb (Optional[ProgressCallback]): ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜

    Returns:
        dict: ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ (output_dir, html_path í¬í•¨). ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬.
    """
    ensure_nltk_resources()
    
    file_name = Path(file_path).name
    bench.start(f"Total Process: {file_name}")

    if progress_cb:
        progress_cb(0.02, f"ğŸ“„ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ë³€í™˜ ì¤‘... ({file_name})")

    # 1. ì…ë ¥ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
    if not os.path.exists(file_path):
        logging.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        if progress_cb:
            progress_cb(1.0, f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ ({file_name})")
        return {}

    # 2. ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    # í´ë”ëª… í˜•ì‹: {íŒŒì¼ëª…}_{ì¶œë°œì–¸ì–´}_to_{ë„ì°©ì–¸ì–´}_{íƒ€ì„ìŠ¤íƒ¬í”„}
    base_filename = Path(file_path).stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("output") / f"{base_filename}_{source_lang}_to_{target_lang}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logging.info(f"[{file_name}] ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ (ì—”ì§„: {engine})")

    # 3. Docling ë³€í™˜
    bench.start(f"Conversion: {file_name}")
    logging.info(f"[{file_name}] ë¬¸ì„œ ë³€í™˜ ì¤‘...")
    try:
        doc: DoclingDocument = converter.convert(file_path).document
    except Exception as e:
        logging.error(f"[{file_name}] ë¬¸ì„œ ë³€í™˜ ì˜¤ë¥˜: {e}", exc_info=True)
        if progress_cb:
            progress_cb(1.0, f"âŒ ì˜¤ë¥˜: ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨ ({file_name})")
        return {}
    bench.end(f"Conversion: {file_name}")
    logging.info(f"[{file_name}] ë¬¸ì„œ ë³€í™˜ ì„±ê³µ.")

    if progress_cb:
        progress_cb(0.20, f"ğŸ“ í…ìŠ¤íŠ¸ ë° ìº¡ì…˜ ì¶”ì¶œ ì¤‘... ({file_name})")

    # 4. í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ë²ˆì—­
    bench.start(f"Translation & Save: {file_name}")
    logging.info(f"[{file_name}] í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ì¼ê´„ ë²ˆì—­ ì¤€ë¹„... (Workers: {max_workers})")

    # --- Phase 1: Collection (í…ìŠ¤íŠ¸ ìˆ˜ì§‘) ---
    all_sentences = []
    doc_items = []
    
    # ë¬¸ì„œë¥¼ ìˆœíšŒí•˜ë©° í…ìŠ¤íŠ¸ ì•„ì´í…œê³¼ ìº¡ì…˜ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    for item, _ in doc.iterate_items():
        doc_items.append((item, _))
        
        if isinstance(item, TextItem):
            if item.text and item.text.strip():
                # NLTKë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                sentences = nltk.sent_tokenize(item.text)
                all_sentences.extend(sentences)
        elif isinstance(item, (TableItem, PictureItem)):
             orig_caption = item.caption_text(doc)
             if orig_caption:
                 all_sentences.append(orig_caption)

    # ì¤‘ë³µ ë¬¸ì¥ ì œê±° (ë²ˆì—­ ë¹„ìš© ì ˆê°)
    unique_sentences = list(set(all_sentences))
    logging.info(f"[{file_name}] ì´ {len(all_sentences)}ê°œ ë¬¸ì¥ ìˆ˜ì§‘ (ê³ ìœ  ë¬¸ì¥: {len(unique_sentences)}ê°œ)")

    if progress_cb:
        progress_cb(0.25, f"ğŸ¤– ë²ˆì—­ ì‹œì‘... ({len(unique_sentences)} ë¬¸ì¥)")

    # --- Phase 2: Translation (ë²ˆì—­) ---
    t_trans_start = time.time()
    
    # ì§„í–‰ë¥  ê³„ì‚°ì„ ìœ„í•œ ìƒìˆ˜ (ë²ˆì—­ ë¹„ì¤‘ 60%)
    TRANSLATE_BASE = 0.25
    TRANSLATE_SPAN = 0.60

    # ë²ˆì—­ ì—”ì§„ì˜ ì§„í–‰ë¥  ì½œë°± ë˜í¼
    def _translate_progress(local_ratio: float, msg: str):
        if progress_cb:
            global_ratio = TRANSLATE_BASE + TRANSLATE_SPAN * local_ratio
            progress_cb(global_ratio, f"ğŸ¤– {msg}")

    # Translator ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¼ê´„ ë²ˆì—­ ì‹¤í–‰
    translator = create_translator(engine)
    translated_results = translator.translate_batch(
        unique_sentences,
        src=source_lang,
        dest=target_lang,
        max_workers=max_workers,
        progress_cb=_translate_progress
    )

    t_trans_end = time.time()
    
    # ì›ë¬¸-ë²ˆì—­ë¬¸ ë§¤í•‘ ìƒì„±
    translation_map = dict(zip(unique_sentences, translated_results))

    # ë²¤ì¹˜ë§ˆí¬ í†µê³„ ê¸°ë¡
    total_chars = sum(len(s) for s in unique_sentences)
    bench.add_stat(
        "Translation (Sentences)",
        t_trans_end - t_trans_start,
        count=len(unique_sentences),
        volume=total_chars,
        unit="chars",
    )
    logging.info(f"[{file_name}] ì¼ê´„ ë²ˆì—­ ì™„ë£Œ ({t_trans_end - t_trans_start:.2f}ì´ˆ)")

    # --- Phase 3: HTML Generation (HTML ìƒì„±) ---
    if progress_cb:
        progress_cb(0.85, f"ğŸ’¾ ê²°ê³¼ íŒŒì¼ ìƒì„± ë° ì´ë¯¸ì§€ ì €ì¥ ì¤‘... ({file_name})")

    path_html = output_dir / f"{base_filename}_interactive.html"
    
    # HTML ìƒì„± ì‹œ ì´ë¯¸ì§€ ì €ì¥ ì§„í–‰ë¥  ë°˜ì˜ (ë‚˜ë¨¸ì§€ 15%)
    GEN_BASE = 0.85
    GEN_SPAN = 0.15
    
    def _gen_progress(local_ratio: float, msg: str):
        if progress_cb:
            global_ratio = GEN_BASE + GEN_SPAN * local_ratio
            progress_cb(global_ratio, f"ğŸ’¾ {msg}")

    html_content = generate_html_content(
        doc,
        doc_items,
        translation_map,
        output_dir,
        base_filename,
        progress_cb=_gen_progress
    )

    with open(path_html, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    if progress_cb:
        progress_cb(1.0, f"âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ({file_name})")
    
    bench.end(f"Translation & Save: {file_name}")
    bench.end(f"Total Process: {file_name}")
    logging.info(f"[{file_name}] íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_dir}")
    
    return {
        "output_dir": output_dir,
        "html_path": path_html
    }

def process_document(
    file_path: str,
    converter: DocumentConverter,
    source_lang: str = "en",
    dest_lang: str = "ko",
    engine: str = "google",
    max_workers: int = 8,
    progress_cb: Optional[ProgressCallback] = None,
) -> dict:
    """
    ì™¸ë¶€(app.py, main.py)ì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•œ í¸ì˜ì„± ë˜í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    process_single_fileì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    return process_single_file(
        file_path=file_path,
        converter=converter,
        source_lang=source_lang,
        target_lang=dest_lang,
        engine=engine,
        max_workers=max_workers,
        progress_cb=progress_cb,
    )
