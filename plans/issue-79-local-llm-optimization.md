# Issue #79: λ΅μ»¬ LLM μ†λ„ μµμ ν™”

## λ©ν‘

ν„μ¬ λ΅μ»¬ λ²μ—­ λ¨λΈ(LFM2, Qwen, Yanolja)μ μ¶”λ΅  μ†λ„λ¥Ό μµλ€ν™”ν•μ—¬ μ‚¬μ©μ κ²½ν—μ„ κ°μ„ ν•©λ‹λ‹¤.

## λ°°κ²½

- ν„μ¬ CPU Only ν™κ²½μ—μ„ LFM2 λ“± GGUF λ¨λΈ μ‚¬μ© μ‹ μ†λ„κ°€ λλ¦Ό
- `llama.cpp` κΈ°λ° μ—”μ§„λ“¤μ μ—°μ‚°λ‰κ³Ό λ©”λ¨λ¦¬ λ€μ—­ν­μ΄ λ³‘λ©
- NLLB μ—”μ§„μ—μ„ CTranslate2 μ μ©μΌλ΅ μ μλ―Έν• μ†λ„ κ°μ„  ν™•μΈλ¨

## μ΅°μ‚¬λ μµμ ν™” λ°©λ²•

### 1. πƒ n_threads μµμ ν™” (μ¦‰μ‹ μ μ© κ°€λ¥, λ‚μ΄λ„: ν•)

**μ›λ¦¬**: ν•μ΄νΌμ¤λ λ”© μ½”μ–΄ μ „μ²΄ μ‚¬μ© μ‹ μ»¨ν…μ¤νΈ μ¤μ„μΉ­μΌλ΅ μ¤νλ ¤ λλ ¤μ§. **λ¬Όλ¦¬ μ½”μ–΄ μ**λ§ ν• λ‹Ήν•λ” κ²ƒμ΄ μµμ .

**μ μ© λ°©λ²•**:
```python
import os
self.llm = Llama(
    model_path=self.model_path,
    n_ctx=4096,
    n_threads=os.cpu_count() // 2,  # λ¬Όλ¦¬ μ½”μ–΄ μλ§ ν• λ‹Ή
    verbose=False
)
```

**μμƒ ν¨κ³Ό**: 10~30% μ†λ„ ν–¥μƒ

---

### 2. π› οΈ AVX2/AVX-512 μµμ ν™” λΉλ“ (μ„¤μΉ μ‹ 1ν, λ‚μ΄λ„: μ¤‘)

**μ›λ¦¬**: κΈ°λ³Έ `pip install`μ€ μµμ ν™” μ—†μ΄ λΉλ“λ¨. AVX2 λ…λ Ήμ–΄ ν™μ„±ν™” μ‹ 2~3λ°° λΉ¨λΌμ§ μ μμ.

**μ μ© λ°©λ²•**:
```powershell
# Windows
pip uninstall llama-cpp-python -y
$env:CMAKE_ARGS = "-DGGML_AVX2=on"
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

```bash
# Linux/Mac
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DGGML_AVX2=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

**μμƒ ν¨κ³Ό**: 2~3λ°° μ†λ„ ν–¥μƒ (CPUμ— λ”°λΌ λ‹¤λ¦„)

---

### 3. π® CUDA GPU κ°€μ† (GPU ν•„μ”, λ‚μ΄λ„: μ¤‘)

**μ›λ¦¬**: NVIDIA GPUκ°€ μλ” κ²½μ° CUDAλ΅ 10~50λ°° λΉ λ¥Έ μ¶”λ΅  κ°€λ¥.

**μ μ© λ°©λ²•**:
```powershell
# Windows (CUDA Toolkit μ„¤μΉ ν•„μ”)
pip uninstall llama-cpp-python -y
$env:CMAKE_ARGS = "-DGGML_CUDA=on"
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

**μ½”λ“ λ³€κ²½**:
```python
self.llm = Llama(
    model_path=self.model_path,
    n_ctx=4096,
    n_gpu_layers=-1,  # λ¨λ“  λ μ΄μ–΄λ¥Ό GPUλ΅
    verbose=False
)
```

**μμƒ ν¨κ³Ό**: 10~50λ°° μ†λ„ ν–¥μƒ

---

### 4. π“¦ μ²­ν¬(Chunk) λ‹¨μ„ λ°°μΉ λ²μ—­ (κµ¬ν„ λ‚μ΄λ„: μƒ)

**μ›λ¦¬**: ν„μ¬ "1λ¬Έμ¥ λ²μ—­ β†’ Reset β†’ 1λ¬Έμ¥ λ²μ—­" λ°©μ‹μ€ μ¤λ²„ν—¤λ“κ°€ νΌ. μ—¬λ¬ λ¬Έμ¥μ„ ν• λ²μ— λ³΄λ‚΄λ©΄ ν¨μ¨μ .

**μ μ© λ°©λ²•**:
```python
def translate_batch_chunked(self, sentences, chunk_size=5):
    results = []
    for i in range(0, len(sentences), chunk_size):
        chunk = sentences[i:i+chunk_size]
        combined = "\n".join([f"<s{j}>{s}</s{j}>" for j, s in enumerate(chunk)])
        result = self.translate(combined, src, dest)
        # κ²°κ³Ό νμ‹±ν•μ—¬ κ°λ³„ λ¬Έμ¥μΌλ΅ λ¶„λ¦¬
        results.extend(parse_chunk_result(result))
    return results
```

**μμƒ ν¨κ³Ό**: 50~200% μ†λ„ ν–¥μƒ (Reset μ¤λ²„ν—¤λ“ μ κ±°)

---

### 5. β΅ CTranslate2 λ³€ν™ (NLLBμ—μ„ κ²€μ¦λ¨, λ‚μ΄λ„: μ¤‘)

**μ›λ¦¬**: CTranslate2λ” Transformer λ¨λΈ μ „μ© μµμ ν™” μ¶”λ΅  μ—”μ§„. INT8 μ–‘μν™” + ν¨μ¨μ  μ—°μ‚°.

**ν„μ¬ μƒνƒ**: NLLB μ—”μ§„μ—μ„ μ΄λ―Έ μ μ©λ¨ (λΉ λ¥Έ μ†λ„ ν™•μΈ)

**λ‹¤λ¥Έ λ¨λΈ μ μ© κ°€λ¥μ„±**:
- LFM2: ν„μ¬ CTranslate2 λ³€ν™ λ²„μ „ μ—†μ (μ§μ ‘ λ³€ν™ ν•„μ”)
- Qwen: CTranslate2 λ―Έμ§€μ›
- Yanolja: CTranslate2 λ―Έμ§€μ›

---

### 6. π“ λ” μ‘μ€ μ–‘μν™” λ¨λΈ μ‚¬μ© (Trade-off, λ‚μ΄λ„: ν•)

**μ›λ¦¬**: Q4_K_M β†’ Q3_K_M, IQ3_XS λ“± λ” μ‘μ€ μ–‘μν™” μ‚¬μ© μ‹ μ†λ„ ν–¥μƒ (ν’μ§ μ €ν• μ„ν—)

**μ μ© λ°©λ²•**: λ¨λΈ νμΌλ…λ§ λ³€κ²½
```python
# ν„μ¬
filename="LFM2-1.2B-Q4_K_M.gguf"
# λ³€κ²½
filename="LFM2-1.2B-Q3_K_M.gguf"  # λ” μ‘μ€ μ–‘μν™”
```

**μμƒ ν¨κ³Ό**: 20~40% μ†λ„ ν–¥μƒ (ν’μ§ μ €ν• κ°€λ¥)

---

### 7. π€ vLLM μ μ© (κ³ κΈ‰, λ‚μ΄λ„: μƒ)

**μ›λ¦¬**: PagedAttention, Continuous Batching λ“± κ³ κΈ‰ μµμ ν™” κΈ°λ²•. μ„λ²„ ν™κ²½μ— μ ν•©.

**μ¥μ **:
- AWQ μ–‘μν™”λ΅ 3λ°° λΉ λ¥Έ μ²λ¦¬λ‰
- λ€κ·λ¨ λ°°μΉ μ²λ¦¬μ— μµμ 

**λ‹¨μ **:
- GPU ν•„μ (VRAM 8GB+)
- λ³µμ΅ν• μ„λ²„ μ„¤μ • ν•„μ”
- ν„μ¬ ν”„λ΅μ νΈ μ•„ν‚¤ν…μ²μ™€ λ§μ§€ μ•μ

---

## μ μ• λ³€κ²½ μ‚¬ν•­ (μ°μ„ μμ„λ³„)

### Phase 1: μ¦‰μ‹ μ μ© (μ½”λ“ λ³€κ²½λ§)

#### [MODIFY] [lfm2.py](file:///c:/github/docling-translate/src/translation/engines/lfm2.py)

- `n_threads` νλΌλ―Έν„° μ¶”κ°€ (λ¬Όλ¦¬ μ½”μ–΄ μ κΈ°λ°)

#### [MODIFY] [lfm2_koen.py](file:///c:/github/docling-translate/src/translation/engines/lfm2_koen.py)

- `n_threads` νλΌλ―Έν„° μ¶”κ°€

#### [MODIFY] [qwen.py](file:///c:/github/docling-translate/src/translation/engines/qwen.py)

- `n_threads` νλΌλ―Έν„° μ¶”κ°€

#### [MODIFY] [yanolja.py](file:///c:/github/docling-translate/src/translation/engines/yanolja.py)

- `n_threads` νλΌλ―Έν„° μ¶”κ°€

---

### Phase 2: λ¬Έμ„ μ•λ‚΄ (README μ—…λ°μ΄νΈ)

#### [MODIFY] README.md, docs/README.en.md

**GPU κ°€μ† μ„¤μΉ μ•λ‚΄ μ¶”κ°€**:
```bash
# CUDA GPU κ°€μ† μ‚¬μ© μ‹ (NVIDIA GPU ν•„μ”)
pip uninstall llama-cpp-python -y
$env:CMAKE_ARGS = "-DGGML_CUDA=on"  # Windows
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

**AVX2 μµμ ν™” λΉλ“ μ•λ‚΄ μ¶”κ°€**:
```bash
# CPU μ„±λ¥ μµμ ν™” λΉλ“
pip uninstall llama-cpp-python -y
$env:CMAKE_ARGS = "-DGGML_AVX2=on"  # Windows
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

---

### Phase 3: ν–¥ν›„ κ³ λ ¤ μ‚¬ν•­ (μ„ νƒμ )

- μ²­ν¬ λ‹¨μ„ λ°°μΉ λ²μ—­ κµ¬ν„ (κµ¬μ΅° λ³€κ²½ ν•„μ”)
- GPU μλ™ κ°μ§€ λ° `n_gpu_layers` μλ™ μ„¤μ •
- λ” μ‘μ€ μ–‘μν™” λ¨λΈ μµμ… μ κ³µ

---

## κ²€μ¦ κ³„ν

### μλ™ ν…μ¤νΈ

**μ‹λ‚λ¦¬μ¤: n_threads μµμ ν™” μ „/ν›„ λΉ„κµ**
1. ν„μ¬ μ½”λ“λ΅ ν‘μ¤€ μƒν”(`samples/1706.03762v7.pdf`) λ²μ—­ μ‹κ°„ μΈ΅μ •
2. `n_threads` μ μ© ν›„ λ™μΌ νμΌλ΅ λ²μ—­ μ‹κ°„ μΈ΅μ •
3. μ†λ„ κ°μ„  λΉ„μ¨ ν™•μΈ

---

## μμƒ ν¨κ³Ό

| μµμ ν™” λ°©λ²• | λ‚μ΄λ„ | μμƒ μ†λ„ ν–¥μƒ | λΉ„κ³  |
|------------|--------|---------------|------|
| n_threads μ„¤μ • | ν• | 10~30% | μ¦‰μ‹ μ μ© κ°€λ¥ |
| AVX2 λΉλ“ | μ¤‘ | 2~3λ°° | μ¬μ„¤μΉ ν•„μ” |
| CUDA GPU | μ¤‘ | 10~50λ°° | GPU ν•„μ” |
| μ²­ν¬ λ°°μΉ | μƒ | 50~200% | κµ¬μ΅° λ³€κ²½ |
| μ‘μ€ μ–‘μν™” | ν• | 20~40% | ν’μ§ μ €ν• κ°€λ¥ |

---

## μ£Όμμ‚¬ν•­

- AVX2 λΉλ“λ” CPUκ°€ AVX2 μ§€μ›ν•΄μ•Ό ν•¨ (2013λ…„ μ΄ν›„ Intel/AMD CPU λ€λ¶€λ¶„ μ§€μ›)
- CUDA λΉλ“λ” CUDA Toolkit μ„¤μΉ λ° νΈν™ λ“λΌμ΄λ²„ ν•„μ”
- μ²­ν¬ λ°°μΉ λ²μ—­μ€ ν”„λ΅¬ν”„νΈ μ„¤κ³„μ™€ νμ‹± λ΅μ§ λ³µμ΅λ„ μ¦κ°€

---

*κ³„ν μ‘μ„±μΌ: 2026-01-01*
